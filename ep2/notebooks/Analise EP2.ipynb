{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0-rc3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://keras.io/examples/nlp/text_classification_with_transformer/\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import KeyedVectors\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "# s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "# s0 = df_to_work['review_text'][1009]\n",
    "# tknzr.tokenize(s0.lower())\n",
    "# s0.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['estou', '.', '#', 'nós', ':(']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = ['estou', '.', 'torcendo', ':)', 'poooooor', ':', 'todos', 'nós', ':(']\n",
    "a[:2]  + [ '#' ]+ a[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo as variáveis do projeto\n",
    "vocab_size = 200000  # Considerar 200k palavras\n",
    "maxlen = 200  # Considerar apenas as 100 primeiras palavras do texto da review\n",
    "\n",
    "embed_dim = 50 # tamanho do Embedding de cada token ( também do word2vec da NILC)\n",
    "num_heads = 2  # N. de cabeças de atenção\n",
    "ff_dim = 32   # tamanho da camada oculta nas redes feed forward dentro do transformer\n",
    "\n",
    "# Path para o arquivo de dados da b2w\n",
    "# B2W_DATAFILE = \"/home/wseidel/workspaces/usp/b2w-reviews01/B2W-Reviews01.csv\"\n",
    "B2W_DATAFILE = \"/home/wesley/workspaces/usp/data/b2w/B2W-Reviews01.csv\"\n",
    "# B2W_DATAFILE = \"/home/wseidel/workspaces/usp/b2w-reviews01/B2W-10k.csv\"\n",
    "\n",
    "\n",
    "# Path para o arquivo de dados de embeddings do NILC\n",
    "# NILC_W2V_DATAFILE = \"/home/wseidel/workspaces/usp/NILC/word2vec_200k.txt\"\n",
    "NILC_W2V_DATAFILE = \"/home/wesley/workspaces/usp/data/nilc/word2vec_200k.txt\"\n",
    "\n",
    "# Quantidade de epocas para o treino\n",
    "QNT_EPOCAS_A_TREINAR = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados a serem analisados\n",
    "b2wCorpus = pd.read_csv(B2W_DATAFILE, sep=';', usecols=[\"review_text\", \"overall_rating\"])\n",
    "\n",
    "# Carregar o Word2Vec do NILC\n",
    "model_w2v = KeyedVectors.load_word2vec_format(NILC_W2V_DATAFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overall_rating</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>47955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_text\n",
       "overall_rating             \n",
       "1                     27369\n",
       "2                      8389\n",
       "3                     16315\n",
       "4                     32345\n",
       "5                     47955"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2wCorpus.groupby(['overall_rating']).count()\n",
    "# b2wCorpus.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train..: 79423 0.6\n",
      "test...: 39712 0.3\n",
      "val....: 13238 0.1\n",
      "----------------------------------------\n",
      "x_train..: 50\n",
      "x_test...: 50\n",
      "x_val....: 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_text_clean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overall_rating</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27369</td>\n",
       "      <td>27369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8389</td>\n",
       "      <td>8389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16315</td>\n",
       "      <td>16315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32345</td>\n",
       "      <td>32345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47955</td>\n",
       "      <td>47955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_text  review_text_clean\n",
       "overall_rating                                \n",
       "0                     27369              27369\n",
       "1                      8389               8389\n",
       "2                     16315              16315\n",
       "3                     32345              32345\n",
       "4                     47955              47955"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_test_val_split(dataset, train_size=0.6, test_size=0.3, colname_stratify='overall_rating',random_seed=29):\n",
    "    val_size = 1 - round((train_size + test_size),1)\n",
    "    split_train_test_size = test_size + val_size\n",
    "\n",
    "    train, val = train_test_split(dataset, \n",
    "                                  test_size=split_train_test_size, \n",
    "                                  stratify=dataset[colname_stratify], \n",
    "                                  random_state=random_seed)\n",
    "\n",
    "    test, val = train_test_split(val, \n",
    "                                  test_size=val_size/split_train_test_size, \n",
    "                                  stratify=val[colname_stratify], \n",
    "                                  random_state=random_seed)\n",
    "    return train.reset_index(), test, val\n",
    "\n",
    "\n",
    "def sentence_to_nilc_index_token(text, stem=False):\n",
    "    # Traduzindo os tokens do B2W para o index do NILC\n",
    "#     tokens = text.lower().split() # Pegar um tokenizer decente...\n",
    "    tokens = tknzr.tokenize(text.lower())\n",
    "    tokens = [model_w2v.vocab[t].index if t in model_w2v.vocab else 19999 for t in tokens ]\n",
    "    return tokens\n",
    "\n",
    "def sort_by_size(df, col_to_sort):\n",
    "    df['sentence_length'] = df[col_to_sort].apply(lambda x: len(x))\n",
    "    df.sort_values(by=['sentence_length'], inplace=True, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def getXY(serieX, serieY, padding_maxlen=50):\n",
    "    x_train = keras.preprocessing.sequence.pad_sequences(train['review_text_clean'], maxlen=padding_maxlen, padding='post')\n",
    "    y_train = train['overall_rating']\n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "# ------ main ----\n",
    "df_to_work = b2wCorpus\n",
    "\n",
    "TAMMAX_SENTENCE=50\n",
    "\n",
    "values_to_retain=[1,2,3,4,5]\n",
    "df_to_work = df_to_work[df_to_work['overall_rating'].isin(values_to_retain)]\n",
    "# df_to_work\n",
    "df_to_work['overall_rating'] = df_to_work.overall_rating.apply(lambda x: x-1)\n",
    "\n",
    "# Aplicando o sentence_to_nilc_index_token\n",
    "df_to_work['review_text_clean'] = df_to_work.review_text.apply(lambda x: sentence_to_nilc_index_token(x))\n",
    "\n",
    "# train, test, val = train_test_val_split(df_to_work, train_size=0.75, test_size=0.15)\n",
    "train, test, val = train_test_val_split(df_to_work)\n",
    "\n",
    "sort_by_size(train, 'review_text_clean')\n",
    "\n",
    "\n",
    "x_train, y_train = getXY(train['review_text_clean'], train['overall_rating'], padding_maxlen=TAMMAX_SENTENCE)\n",
    "x_test,  y_test  = getXY(test['review_text_clean'], test['overall_rating'], padding_maxlen=TAMMAX_SENTENCE)\n",
    "x_val,   y_val   = getXY(val['review_text_clean'], val['overall_rating'], padding_maxlen=TAMMAX_SENTENCE)\n",
    "\n",
    "\n",
    "print(\"train..:\", len(train), round(len(train) / len(df_to_work),3) ) \n",
    "print(\"test...:\", len(test), round(len(test) / len(df_to_work),3) )\n",
    "print(\"val....:\", len(val), round(len(val) / len(df_to_work),3) )\n",
    "print(\"--\" * 20) \n",
    "print(\"x_train..:\", len(x_train[-1]), ) \n",
    "print(\"x_test...:\", len(x_test[-1]), ) \n",
    "print(\"x_val....:\", len(x_val[-1]), ) \n",
    "# train = train.reset_index(drop=True)\n",
    "# train = train.reset_index(inplace=True)\n",
    "# train = train.copy()\n",
    "\n",
    "# df_to_work.groupby\n",
    "df_to_work.groupby(['overall_rating']).count()\n",
    "# b2wCorpus.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qnt dados..: 19\n",
      "lote size..: 3\n",
      "lote count..: 7\n",
      "Pegando lote 0 de 7:[0, 1, 2]\n",
      "Pegando lote 1 de 7:[3, 4, 5]\n",
      "Pegando lote 2 de 7:[6, 7, 8]\n",
      "Pegando lote 3 de 7:[9, 10, 11]\n",
      "Pegando lote 4 de 7:[12, 13, 14]\n",
      "Pegando lote 5 de 7:[15, 16, 17]\n",
      "Pegando lote 6 de 7:[18]\n"
     ]
    }
   ],
   "source": [
    "dados = list(range(19))\n",
    "lote_size = 3\n",
    "lote_count = int(np.ceil(len(dados)/ lote_size))\n",
    "print(\"qnt dados..:\", len(dados))\n",
    "print(\"lote size..:\", lote_size)\n",
    "print(\"lote count..:\", lote_count)\n",
    "for i in range(0,lote_count):\n",
    "    print(f\"Pegando lote {i} de {lote_count}:\", end=\"\")\n",
    "    print(dados[ i*lote_size : i*lote_size+lote_size ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 50, 50)            10000050  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                10624     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 10,010,839\n",
      "Trainable params: 10,010,839\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras import Sequential\n",
    "# from keras.utils import Sequence\n",
    "# from keras.layers import LSTM, Dense, Masking\n",
    "# import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "# model = tf.keras.Sequential([\n",
    "from tensorflow import keras\n",
    "\n",
    "def get_lstm_model(dropout_prob=0.0):\n",
    "    embedding_layer = model_w2v.get_keras_embedding()\n",
    "#     embedding_layer.trainable = True\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(TAMMAX_SENTENCE, )))\n",
    "#     model.add(embedding_layer)\n",
    "    model.add(layers.Embedding(200001, 50, input_length=TAMMAX_SENTENCE))\n",
    "    model.add(layers.LSTM(32))\n",
    "    model.add(layers.Dropout(dropout_prob))\n",
    "    model.add(keras.layers.Dense(5, activation='softmax'))\n",
    "    model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = get_lstm_model(0.5)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = 'm1_lstm_drop0.0'\n",
    "# model = get_lstm_model(dropout_prob=0.5)\n",
    "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "# mc = ModelCheckpoint('../model_data/' + name + 'best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "# history = model.fit(x_train, y_train, batch_size=64, epochs=100, validation_data=(x_val, y_val), callbacks=[es, mc])\n",
    "# save_history(history, name)\n",
    "# model.evaluate(x_test, y_test)\n",
    "# display_loss_plot(history, name)\n",
    "# display_acc_plot(history, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2482/2482 [==============================] - 485s 195ms/step - loss: 1.0702 - accuracy: 0.5392 - val_loss: 0.9313 - val_accuracy: 0.5990\n",
      "Epoch 2/2\n",
      "2482/2482 [==============================] - 493s 199ms/step - loss: 0.9354 - accuracy: 0.5964 - val_loss: 0.8605 - val_accuracy: 0.6244\n",
      "2482/2482 [==============================] - 14s 6ms/step - loss: 0.8605 - accuracy: 0.6244\n",
      "Loss:  0.8605464100837708\n",
      "Accuracy:  0.6244035363197327\n"
     ]
    }
   ],
   "source": [
    "# Ver lista06\n",
    "\n",
    "# Ler aqui pro batch generator:\n",
    "#     https://datascience.stackexchange.com/questions/48796/how-to-feed-lstm-with-different-input-array-sizes\n",
    "\n",
    "# Seu código aqui\n",
    "\n",
    "QNT_EPOCAS_TREINO = 2\n",
    "\n",
    "\n",
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=QNT_EPOCAS_TREINO, validation_data=(x_val, y_val)\n",
    ")\n",
    "\n",
    "loss, accuracy = model.evaluate(x=x_test,y=y_test)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparações \n",
    "\n",
    "# Dropout 0.0\n",
    "\n",
    "### embedding_layer.trainable = True\n",
    "```\n",
    "Epoch 1/10\n",
    "2482/2482 [==============================] - 383s 154ms/step - loss: 1.0851 - accuracy: 0.5380 - val_loss: 0.9881 - val_accuracy: 0.5780\n",
    "Epoch 2/10\n",
    "2482/2482 [==============================] - 400s 161ms/step - loss: 0.9654 - accuracy: 0.5859 - val_loss: 0.9020 - val_accuracy: 0.6110\n",
    "Epoch 3/10\n",
    "2482/2482 [==============================] - 391s 157ms/step - loss: 0.9116 - accuracy: 0.6084 - val_loss: 0.8640 - val_accuracy: 0.6291\n",
    "Epoch 4/10\n",
    "2482/2482 [==============================] - 412s 166ms/step - loss: 0.8630 - accuracy: 0.6309 - val_loss: 0.7967 - val_accuracy: 0.6648\n",
    "Epoch 5/10\n",
    "2482/2482 [==============================] - 404s 163ms/step - loss: 0.8152 - accuracy: 0.6520 - val_loss: 0.7535 - val_accuracy: 0.6912\n",
    "Epoch 6/10\n",
    "2482/2482 [==============================] - 425s 171ms/step - loss: 0.7649 - accuracy: 0.6772 - val_loss: 0.6945 - val_accuracy: 0.7154\n",
    "Epoch 7/10\n",
    "2482/2482 [==============================] - 478s 193ms/step - loss: 0.7166 - accuracy: 0.6992 - val_loss: 0.6500 - val_accuracy: 0.7313\n",
    "Epoch 8/10\n",
    "2482/2482 [==============================] - 492s 198ms/step - loss: 0.6680 - accuracy: 0.7196 - val_loss: 0.6138 - val_accuracy: 0.7464\n",
    "Epoch 9/10\n",
    "2482/2482 [==============================] - 481s 194ms/step - loss: 0.6248 - accuracy: 0.7370 - val_loss: 0.5679 - val_accuracy: 0.7631\n",
    "Epoch 10/10\n",
    "2482/2482 [==============================] - 472s 190ms/step - loss: 0.5873 - accuracy: 0.7511 - val_loss: 0.5323 - val_accuracy: 0.7761\n",
    "2482/2482 [==============================] - 29s 12ms/step - loss: 0.5323 - accuracy: 0.7761\n",
    "Loss:  0.532257080078125\n",
    "Accuracy:  0.7761353850364685\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### embedding_layer.trainable = False\n",
    "```\n",
    "Epoch 1/10\n",
    "2482/2482 [==============================] - 90s 36ms/step - loss: 1.2016 - accuracy: 0.4954 - val_loss: 1.1219 - val_accuracy: 0.5238\n",
    "Epoch 2/10\n",
    "2482/2482 [==============================] - 88s 35ms/step - loss: 1.1006 - accuracy: 0.5322 - val_loss: 1.0658 - val_accuracy: 0.5467\n",
    "Epoch 3/10\n",
    "2482/2482 [==============================] - 89s 36ms/step - loss: 1.0550 - accuracy: 0.5500 - val_loss: 1.0191 - val_accuracy: 0.5655\n",
    "Epoch 4/10\n",
    "2482/2482 [==============================] - 92s 37ms/step - loss: 1.0298 - accuracy: 0.5603 - val_loss: 1.0086 - val_accuracy: 0.5682\n",
    "Epoch 5/10\n",
    "2482/2482 [==============================] - 94s 38ms/step - loss: 1.0102 - accuracy: 0.5693 - val_loss: 0.9865 - val_accuracy: 0.5761\n",
    "Epoch 6/10\n",
    "2482/2482 [==============================] - 91s 37ms/step - loss: 0.9939 - accuracy: 0.5742 - val_loss: 0.9819 - val_accuracy: 0.5804\n",
    "Epoch 7/10\n",
    "2482/2482 [==============================] - 88s 35ms/step - loss: 0.9813 - accuracy: 0.5799 - val_loss: 0.9750 - val_accuracy: 0.5857\n",
    "Epoch 8/10\n",
    "2482/2482 [==============================] - 87s 35ms/step - loss: 0.9705 - accuracy: 0.5850 - val_loss: 0.9909 - val_accuracy: 0.5775\n",
    "Epoch 9/10\n",
    "2482/2482 [==============================] - 84s 34ms/step - loss: 0.9597 - accuracy: 0.5883 - val_loss: 0.9482 - val_accuracy: 0.5949\n",
    "Epoch 10/10\n",
    "2482/2482 [==============================] - 85s 34ms/step - loss: 0.9503 - accuracy: 0.5919 - val_loss: 0.9388 - val_accuracy: 0.5954\n",
    "2482/2482 [==============================] - 26s 11ms/step - loss: 0.9388 - accuracy: 0.5954\n",
    "\n",
    "No teste:\n",
    "Loss:  0.9387642741203308\n",
    "Accuracy:  0.5953942537307739\n",
    "```\n",
    "\n",
    "\n",
    "###  embedding_layer.trainable = False e o UNK sendo -19999\n",
    "```\n",
    "Epoch 1/10\n",
    "2482/2482 [==============================] - 138s 56ms/step - loss: 1.2058 - accuracy: 0.4917 - val_loss: 1.1347 - val_accuracy: 0.5199\n",
    "Epoch 2/10\n",
    "2482/2482 [==============================] - 129s 52ms/step - loss: 1.1054 - accuracy: 0.5303 - val_loss: 1.0637 - val_accuracy: 0.5500\n",
    "Epoch 3/10\n",
    "2482/2482 [==============================] - 128s 52ms/step - loss: 1.0628 - accuracy: 0.5495 - val_loss: 1.0425 - val_accuracy: 0.5602\n",
    "Epoch 4/10\n",
    "2482/2482 [==============================] - 125s 51ms/step - loss: 1.0364 - accuracy: 0.5590 - val_loss: 1.0186 - val_accuracy: 0.5630\n",
    "Epoch 5/10\n",
    " 143/2482 [>.............................] - ETA: 1:31 - loss: 1.0195 - accuracy: 0.5660\n",
    " ```\n",
    "\n",
    "###  [embedding_layer.trainable = False] e o [UNK = pos -19999] (nltk tokenizer twitter)\n",
    "``` \n",
    "Epoch 1/10\n",
    "2482/2482 [==============================] - 187s 75ms/step - loss: 1.1684 - accuracy: 0.5067 - val_loss: 1.0854 - val_accuracy: 0.5410\n",
    "Epoch 2/10\n",
    "2482/2482 [==============================] - 172s 69ms/step - loss: 1.0489 - accuracy: 0.5517 - val_loss: 1.0070 - val_accuracy: 0.5755\n",
    "Epoch 3/10\n",
    "2482/2482 [==============================] - 169s 68ms/step - loss: 0.9994 - accuracy: 0.5731 - val_loss: 0.9720 - val_accuracy: 0.5846\n",
    "Epoch 4/10\n",
    "2482/2482 [==============================] - 129s 52ms/step - loss: 0.9737 - accuracy: 0.5837 - val_loss: 0.9497 - val_accuracy: 0.5915\n",
    "Epoch 5/10\n",
    "2482/2482 [==============================] - 87s 35ms/step - loss: 0.9521 - accuracy: 0.5920 - val_loss: 0.9395 - val_accuracy: 0.5960\n",
    "Epoch 6/10\n",
    "2482/2482 [==============================] - 86s 35ms/step - loss: 0.9380 - accuracy: 0.5963 - val_loss: 0.9266 - val_accuracy: 0.6064\n",
    "Epoch 7/10\n",
    "2482/2482 [==============================] - 119s 48ms/step - loss: 0.9238 - accuracy: 0.6011 - val_loss: 0.8999 - val_accuracy: 0.6130\n",
    "Epoch 8/10\n",
    "2482/2482 [==============================] - 132s 53ms/step - loss: 0.9126 - accuracy: 0.6051 - val_loss: 0.8931 - val_accuracy: 0.6152\n",
    "Epoch 9/10\n",
    "2482/2482 [==============================] - 119s 48ms/step - loss: 0.9021 - accuracy: 0.6116 - val_loss: 0.8847 - val_accuracy: 0.6161\n",
    "Epoch 10/10\n",
    "2482/2482 [==============================] - 125s 50ms/step - loss: 0.8907 - accuracy: 0.6145 - val_loss: 0.8713 - val_accuracy: 0.6226\n",
    "2482/2482 [==============================] - 30s 12ms/step - loss: 0.8713 - accuracy: 0.6226\n",
    "Loss:  0.8712893724441528\n",
    "Accuracy:  0.6226407885551453\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "# Embedding Zerado com 200k posicoes (dropout=0) (LSTM=64)\n",
    "\n",
    "```\n",
    "Epoch 1/10\n",
    "2482/2482 [==============================] - 524s 211ms/step - loss: 1.0436 - accuracy: 0.5506 - val_loss: 0.9006 - val_accuracy: 0.6124\n",
    "Epoch 2/10\n",
    "2482/2482 [==============================] - 488s 197ms/step - loss: 0.9064 - accuracy: 0.6094 - val_loss: 0.8524 - val_accuracy: 0.6254\n",
    "Epoch 3/10\n",
    "2482/2482 [==============================] - 478s 193ms/step - loss: 0.8461 - accuracy: 0.6357 - val_loss: 0.7751 - val_accuracy: 0.6721\n",
    "Epoch 4/10\n",
    "2482/2482 [==============================] - 480s 194ms/step - loss: 0.7888 - accuracy: 0.6620 - val_loss: 0.7151 - val_accuracy: 0.7057\n",
    "Epoch 5/10\n",
    "2482/2482 [==============================] - 481s 194ms/step - loss: 0.7322 - accuracy: 0.6890 - val_loss: 0.6723 - val_accuracy: 0.7196\n",
    "Epoch 6/10\n",
    "2482/2482 [==============================] - 482s 194ms/step - loss: 0.6794 - accuracy: 0.7124 - val_loss: 0.6141 - val_accuracy: 0.7455\n",
    "Epoch 7/10\n",
    "2482/2482 [==============================] - 481s 194ms/step - loss: 0.6276 - accuracy: 0.7360 - val_loss: 0.5545 - val_accuracy: 0.7696\n",
    "Epoch 8/10\n",
    "2482/2482 [==============================] - 483s 194ms/step - loss: 0.5787 - accuracy: 0.7553 - val_loss: 0.5126 - val_accuracy: 0.7876\n",
    "Epoch 9/10\n",
    "2482/2482 [==============================] - 482s 194ms/step - loss: 0.5377 - accuracy: 0.7725 - val_loss: 0.4754 - val_accuracy: 0.8013\n",
    "Epoch 10/10\n",
    "2482/2482 [==============================] - 483s 195ms/step - loss: 0.5005 - accuracy: 0.7872 - val_loss: 0.4457 - val_accuracy: 0.8152\n",
    "2482/2482 [==============================] - 24s 10ms/step - loss: 0.4457 - accuracy: 0.8152\n",
    "Loss:  0.44574812054634094\n",
    "Accuracy:  0.8152424097061157\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "# Embedding Zerado com 200k posicoes (dropout 0.5) (LSTM =32)\n",
    "\n",
    "```\n",
    "Epoch 1/2\n",
    "2482/2482 [==============================] - 485s 195ms/step - loss: 1.0702 - accuracy: 0.5392 - val_loss: 0.9313 - val_accuracy: 0.5990\n",
    "Epoch 2/2\n",
    "2482/2482 [==============================] - 493s 199ms/step - loss: 0.9354 - accuracy: 0.5964 - val_loss: 0.8605 - val_accuracy: 0.6244\n",
    "2482/2482 [==============================] - 14s 6ms/step - loss: 0.8605 - accuracy: 0.6244\n",
    "Loss:  0.8605464100837708\n",
    "Accuracy:  0.6244035363197327\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d801def19b4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# y_test[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "y_pred[0]\n",
    "# y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
