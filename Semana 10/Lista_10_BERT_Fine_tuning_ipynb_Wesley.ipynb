{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "723yl-dYmrS2"
   },
   "source": [
    "<img src=\"https://github.com/alan-barzilay/NLPortugues/blob/master/imagens/logo_nlportugues.png?raw=true\"  style=\"height:65%\" align=\"right\">\n",
    "\n",
    "\n",
    "# Lista10 - BERT\n",
    "**Nome:**  Wesley Seidel Carvalho\n",
    "\n",
    "**Numero Usp:** 6544342\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "______________\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elUy3yJQmrS9"
   },
   "source": [
    "\n",
    "O objetivo desta lista é fazer com que vocês se familiarizem com o BERT por meio da biblioteca HuggingFace. Novamente, as questões 1 2 e 3 podem ser copiadas de listas anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkCzL_OHmrTB",
    "outputId": "6fe55b93-f67b-4a6d-d118-625c94044d37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-3.5.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 556 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages (from transformers) (3.13.0)\n",
      "Collecting sentencepiece==0.1.91\n",
      "  Downloading sentencepiece-0.1.91-cp38-cp38-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers==0.9.3\n",
      "  Downloading tokenizers-0.9.3-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages (from transformers) (2020.10.23)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: requests in /mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: numpy in /mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: six in /mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: setuptools in /mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages (from protobuf->transformers) (50.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages (from requests->transformers) (3.0.4)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=11db6d01e7f67db541c02ec209793ef97c0bf53965c0b04313e086649374b911\n",
      "  Stored in directory: /mnt/dados/home/wseidel/.cache/pip/wheels/7b/78/f4/27d43a65043e1b75dbddaa421b573eddc67e712be4b1c80677\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, sentencepiece, tokenizers, filelock, transformers\n",
      "Successfully installed filelock-3.0.12 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/mnt/dados/home/wseidel/envs/usp-MAC5725c/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vr5lAyHfmrTc",
    "outputId": "0a9cd2e4-38b5-4aa1-bebb-1b358eece44a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yntL_jNRmrTp"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    TFBertForSequenceClassification,\n",
    "    TFTrainer,\n",
    "    TFTrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMWZdtOOmrT2"
   },
   "source": [
    "### Importando os dados como um dataframe\n",
    "\n",
    "Para esta lista nós utilizaremos o dataset **B2W-Reviews01** que consiste em avaliações de mais de 130k compras online no site Americanas.com e [esta disponivel no github](https://github.com/b2wdigital/b2w-reviews01) sob a licensa CC BY-NC-SA 4.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "B4pGXAmSmrT4",
    "outputId": "5cd25b0f-ef72-4d7c-d460-9dcefc1132f4",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>site_category_lv1</th>\n",
       "      <th>site_category_lv2</th>\n",
       "      <th>review_title</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>recommend_to_a_friend</th>\n",
       "      <th>review_text</th>\n",
       "      <th>reviewer_birth_year</th>\n",
       "      <th>reviewer_gender</th>\n",
       "      <th>reviewer_state</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:11:28</td>\n",
       "      <td>d0fb1ca69422530334178f5c8624aa7a99da47907c44de...</td>\n",
       "      <td>132532965</td>\n",
       "      <td>Notebook Asus Vivobook Max X541NA-GO472T Intel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Informática</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>Bom</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Estou contente com a compra entrega rápida o ú...</td>\n",
       "      <td>1958</td>\n",
       "      <td>F</td>\n",
       "      <td>RJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 00:13:48</td>\n",
       "      <td>014d6dc5a10aed1ff1e6f349fb2b059a2d3de511c7538a...</td>\n",
       "      <td>22562178</td>\n",
       "      <td>Copo Acrílico Com Canudo 500ml Rocie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Utilidades Domésticas</td>\n",
       "      <td>Copos, Taças e Canecas</td>\n",
       "      <td>Preço imbatível, ótima qualidade</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Por apenas R$1994.20,eu consegui comprar esse ...</td>\n",
       "      <td>1996</td>\n",
       "      <td>M</td>\n",
       "      <td>SC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 00:26:02</td>\n",
       "      <td>44f2c8edd93471926fff601274b8b2b5c4824e386ae4f2...</td>\n",
       "      <td>113022329</td>\n",
       "      <td>Panela de Pressão Elétrica Philips Walita Dail...</td>\n",
       "      <td>philips walita</td>\n",
       "      <td>Eletroportáteis</td>\n",
       "      <td>Panela Elétrica</td>\n",
       "      <td>ATENDE TODAS AS EXPECTATIVA.</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...</td>\n",
       "      <td>1984</td>\n",
       "      <td>M</td>\n",
       "      <td>SP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 00:35:54</td>\n",
       "      <td>ce741665c1764ab2d77539e18d0e4f66dde6213c9f0863...</td>\n",
       "      <td>113851581</td>\n",
       "      <td>Betoneira Columbus - Roma Brinquedos</td>\n",
       "      <td>roma jensen</td>\n",
       "      <td>Brinquedos</td>\n",
       "      <td>Veículos de Brinquedo</td>\n",
       "      <td>presente mais que desejado</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...</td>\n",
       "      <td>1985</td>\n",
       "      <td>F</td>\n",
       "      <td>SP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 01:00:28</td>\n",
       "      <td>7d7b6b18dda804a897359276cef0ca252f9932bf4b5c8e...</td>\n",
       "      <td>131788803</td>\n",
       "      <td>Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com C...</td>\n",
       "      <td>lg</td>\n",
       "      <td>TV e Home Theater</td>\n",
       "      <td>TV</td>\n",
       "      <td>Sem duvidas, excelente</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>A entrega foi no prazo, as americanas estão de...</td>\n",
       "      <td>1994</td>\n",
       "      <td>M</td>\n",
       "      <td>MG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       submission_date                                        reviewer_id  \\\n",
       "0  2018-01-01 00:11:28  d0fb1ca69422530334178f5c8624aa7a99da47907c44de...   \n",
       "1  2018-01-01 00:13:48  014d6dc5a10aed1ff1e6f349fb2b059a2d3de511c7538a...   \n",
       "2  2018-01-01 00:26:02  44f2c8edd93471926fff601274b8b2b5c4824e386ae4f2...   \n",
       "3  2018-01-01 00:35:54  ce741665c1764ab2d77539e18d0e4f66dde6213c9f0863...   \n",
       "4  2018-01-01 01:00:28  7d7b6b18dda804a897359276cef0ca252f9932bf4b5c8e...   \n",
       "\n",
       "   product_id                                       product_name  \\\n",
       "0   132532965  Notebook Asus Vivobook Max X541NA-GO472T Intel...   \n",
       "1    22562178               Copo Acrílico Com Canudo 500ml Rocie   \n",
       "2   113022329  Panela de Pressão Elétrica Philips Walita Dail...   \n",
       "3   113851581               Betoneira Columbus - Roma Brinquedos   \n",
       "4   131788803  Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com C...   \n",
       "\n",
       "    product_brand      site_category_lv1       site_category_lv2  \\\n",
       "0             NaN            Informática                Notebook   \n",
       "1             NaN  Utilidades Domésticas  Copos, Taças e Canecas   \n",
       "2  philips walita        Eletroportáteis         Panela Elétrica   \n",
       "3     roma jensen             Brinquedos   Veículos de Brinquedo   \n",
       "4              lg      TV e Home Theater                      TV   \n",
       "\n",
       "                       review_title  overall_rating recommend_to_a_friend  \\\n",
       "0                               Bom               4                   Yes   \n",
       "1  Preço imbatível, ótima qualidade               4                   Yes   \n",
       "2      ATENDE TODAS AS EXPECTATIVA.               4                   Yes   \n",
       "3        presente mais que desejado               4                   Yes   \n",
       "4            Sem duvidas, excelente               5                   Yes   \n",
       "\n",
       "                                         review_text reviewer_birth_year  \\\n",
       "0  Estou contente com a compra entrega rápida o ú...                1958   \n",
       "1  Por apenas R$1994.20,eu consegui comprar esse ...                1996   \n",
       "2  SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...                1984   \n",
       "3  MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...                1985   \n",
       "4  A entrega foi no prazo, as americanas estão de...                1994   \n",
       "\n",
       "  reviewer_gender reviewer_state Unnamed: 14 Unnamed: 15 Unnamed: 16  \\\n",
       "0               F             RJ         NaN         NaN         NaN   \n",
       "1               M             SC         NaN         NaN         NaN   \n",
       "2               M             SP         NaN         NaN         NaN   \n",
       "3               F             SP         NaN         NaN         NaN   \n",
       "4               M             MG         NaN         NaN         NaN   \n",
       "\n",
       "  Unnamed: 17 Unnamed: 18  \n",
       "0         NaN         NaN  \n",
       "1         NaN         NaN  \n",
       "2         NaN         NaN  \n",
       "3         NaN         NaN  \n",
       "4         NaN         NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2wCorpus = pd.read_csv(\"https://raw.githubusercontent.com/abarbosa94/NLPortugues/master/Semana%2009/data/b2w-10k.csv\")\n",
    "b2wCorpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQdq2kVqmrUG",
    "outputId": "69507d23-e13d-4b84-e729-d09e0ee29464"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Estou contente com a compra entrega rápida o ú...\n",
       "1       Por apenas R$1994.20,eu consegui comprar esse ...\n",
       "2       SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...\n",
       "3       MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...\n",
       "4       A entrega foi no prazo, as americanas estão de...\n",
       "                              ...                        \n",
       "9994    Celular muito rápido, com processador e armaze...\n",
       "9995    achei o produto muito frágil, o material veio ...\n",
       "9996    Uma porcaria pois ñ recebi ñ recomendo pra nin...\n",
       "9997    Maquina excelente,super pratica. recomendo.ent...\n",
       "9998    Agradeço pelo compromisso, obrigado. ,...........\n",
       "Name: review_text, Length: 9999, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2wCorpus[\"review_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nq_DEzCJmrUZ"
   },
   "source": [
    "\n",
    "### Pré-processamento\n",
    "Seria util nos livrarmos das colunas que não são relevantes para o nosso problema e tambem verificar se não tem nada de esquisito nas colunas que vamos utilizar. \n",
    "Por exemplo, se fossemos utilizar a coluna \"reviewer_gender\" nós precisariamos nos livrar desses valores esquisitos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BsFaqFYXmrUe",
    "outputId": "5b52c94b-0385-40ba-d66a-216463771694"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M                                                                         5033\n",
       "F                                                                         4624\n",
       "1970                                                                         1\n",
       "Ocorrência: Z-Devolução Em Andamento Ao Cd de São Paulo 22/12/17 16:12       1\n",
       "                                                                             1\n",
       "Name: reviewer_gender, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2wCorpus[\"reviewer_gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q61JFx2zmrUz"
   },
   "source": [
    "## <font color='blue'>Questão 1 </font>\n",
    "\n",
    "a) Selecione apenas as colunas relevantes: \"review_text\" e \"recommend_to_a_friend\". \n",
    "\n",
    "b) Converta a coluna \"recommend_to_a_friend\" de uma coluna de `str` para uma coluna de `int`:\n",
    "\n",
    "- \"Yes\"-> 1\n",
    "- \"No\" -> 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "KHCrVJjLmrU1",
    "outputId": "3c06304b-47a3-46c5-b41b-5beccac0906b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>recommend_to_a_friend</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recommend_to_a_friend_new</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2548</td>\n",
       "      <td>2548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7451</td>\n",
       "      <td>7451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           review_text  recommend_to_a_friend\n",
       "recommend_to_a_friend_new                                    \n",
       "0                                 2548                   2548\n",
       "1                                 7451                   7451"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seu código aqui\n",
    "df = b2wCorpus[[\"review_text\", \"recommend_to_a_friend\"]]\n",
    "df = df.assign(recommend_to_a_friend_new=0)\n",
    "df['recommend_to_a_friend_new'] = df.recommend_to_a_friend.apply(lambda word : 1 if word == 'Yes' else 0)\n",
    "\n",
    "# df.head()\n",
    "df.groupby(by=\"recommend_to_a_friend_new\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yo1EtKImrVH"
   },
   "source": [
    "### Separando em teste e treino\n",
    "## <font color='blue'>Questão 2 </font>\n",
    "\n",
    "Agora com o dataset já pré-processado, separe o em 2 partes, um conjunto de teste e um conjunto de treino. Novamente você pode utilizar a função [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) do Scikit-Learn como na lista passada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AYD_mLIXmrVJ"
   },
   "outputs": [],
   "source": [
    "# Seu código aqui\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['review_text'].values, df['recommend_to_a_friend_new'].values, test_size=0.20) # , random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKkKN8m4mrVb"
   },
   "source": [
    "### Tokenizando\n",
    "\n",
    "Para aplicar o processo de _Tokenize_ dos nossos dados, diferente das listas anteriores, utilizaremos a classe [BertTokenizer](https://huggingface.co/transformers/master/model_doc/bert.html#berttokenizer) da biblioteca [transformers](https://github.com/huggingface/transformers) do HuggingFace.\n",
    "\n",
    "Para isso, veja o exemplo abaixo:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ra_E9gMmrWb"
   },
   "source": [
    "Como é possível ver, o resultado gera um dicionário com 3 chaves, representando diferentes tensores:\n",
    "\n",
    "    - input_ids (os arrays de entrada convertidos para inteiro)\n",
    "    - token_type_ids (Indicação se pertence a sentença A ou B [0 é sentença A e 1 é sentença B])\n",
    "    - attention_mask (indicando quais tokens foram mascarados. Como todos os tokens **não** foram mascarados, o valor aqui sempre é 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3SYgHWgmrWk"
   },
   "source": [
    "## <font color='blue'>Questão 3 </font>\n",
    "\n",
    "Aplique o tokenizer nos dados de treino e teste, gerando duas variáveis **encoded_train** e **encoded_test**, considerando o max_length como o tamanho da sentença ideal. Plotamos um histograma do comprimento dos reviews para lhe auxiliar nessa decisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "Kk2klz9lmrWo",
    "outputId": "0920882c-b071-4daf-ae6c-317868322df4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(linha.split()) for linha in b2wCorpus[\"review_text\"]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "Je6EI5y9P886",
    "outputId": "a4c5edd3-5092-4b2a-80f6-5af1859ff3fd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59128c7c91dd4747984d37ab1c3117cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=209528.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3076dfef14e48648deb9a7c5a9aba65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457860459f0441fd94cf17b36bd36425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=112.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a72b9ce01646678c9fb1657d9c63e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=43.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "REF_MODEL = 'neuralmind/bert-base-portuguese-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(REF_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYMgxlBcQCrb"
   },
   "outputs": [],
   "source": [
    "# text.values.tolist()\n",
    "# x_train.tolist()\n",
    "# [ [t] for t in x_train[0:10]]\n",
    "# x_train[0:10]\n",
    "# np.expand_dims(x_train[0:10], axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qOC9BO1ymrW2"
   },
   "outputs": [],
   "source": [
    "# Seu código aqui\n",
    "SEQ_LEN=50\n",
    "\n",
    "encoded_train = tokenizer(\n",
    "    np.expand_dims(x_train, axis=1).tolist(),\n",
    "    text_pair=None,\n",
    "    is_split_into_words=True,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=SEQ_LEN,\n",
    "    pad_to_max_length=True,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "# encoded_text_labels = np.array([0,0,1])\n",
    "# encoded_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MsPmBJvemrXC"
   },
   "outputs": [],
   "source": [
    "# Seu código aqui\n",
    "encoded_test =  tokenizer(\n",
    "    np.expand_dims(x_test, axis=1).tolist(),\n",
    "    text_pair=None,\n",
    "    is_split_into_words=True,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=SEQ_LEN,\n",
    "    pad_to_max_length=True,\n",
    "    return_tensors='tf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "JWWAeovrmrXO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape do treino: (7999, 50)\n",
      "Shape do teste.: (2000, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( \"Shape do treino:\", encoded_train['input_ids'].shape)\n",
    "print( \"Shape do teste.:\", encoded_test['input_ids'].shape)\n",
    "encoded_test.keys()\n",
    "# print( \"Shape do teste.:\", encoded_test['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rTBlaNRmrXc"
   },
   "source": [
    "### Montando o modelo\n",
    "\n",
    "Para montar o modelo, iremos utilizar a classe TFBertForSequenceClassification, do HuggingFace\n",
    "\n",
    "Aqui tem um exemplo de código para vocês seguirem!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FusDr7AEmrYS"
   },
   "source": [
    "## Treinando e avaliando seu modelo\n",
    "\n",
    "###  <font color='blue'>Questão 4 </font>\n",
    "\n",
    "Defina e treine seu modelo.\n",
    "\n",
    "**Lembre-se de tambem adicionar os dados de validação do modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.7.0-cp38-cp38-manylinux1_x86_64.whl (776.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 776.8 MB 42 bytes/s a 0:00:01     |█████                           | 121.7 MB 3.3 MB/s eta 0:03:19     |█████▏                          | 125.3 MB 2.7 MB/s eta 0:04:06     |███████▍                        | 178.7 MB 2.7 MB/s eta 0:03:42     |█████████                       | 218.3 MB 3.4 MB/s eta 0:02:44     |█████████▌                      | 231.7 MB 3.2 MB/s eta 0:02:52     |██████████████████████████████▌ | 740.6 MB 4.1 MB/s eta 0:00:09     |██████████████████████████████▋ | 742.1 MB 4.1 MB/s eta 0:00:09\n",
      "\u001b[?25hRequirement already satisfied: numpy in /mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages (from torch) (1.18.5)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Collecting future\n",
      "  Using cached future-0.18.2.tar.gz (829 kB)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=1484c7ffa4b559b51170c6899d2218b0ac5625a239cff0a2bb2c8003faebe252\n",
      "  Stored in directory: /mnt/dados/home/wseidel/.cache/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n",
      "Successfully built future\n",
      "Installing collected packages: typing-extensions, dataclasses, future, torch\n",
      "Successfully installed dataclasses-0.6 future-0.18.2 torch-1.7.0 typing-extensions-3.7.4.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/mnt/dados/home/wseidel/envs/usp-MAC5725c/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "6GshFvxOL5Xh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_token (InputLayer)        [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masked_token (InputLayer)       [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_for_sequence_classifica ((None, 2),)         108924674   input_token[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 108,924,674\n",
      "Trainable params: 108,924,674\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertForSequenceClassification.from_pretrained(REF_MODEL, from_pt=True, num_labels=2)\n",
    "input_ids = tf.keras.layers.Input(shape=(SEQ_LEN,), name='input_token', dtype='int32')\n",
    "input_masks_ids = tf.keras.layers.Input(shape=(SEQ_LEN,), name='masked_token', dtype='int32')\n",
    "\n",
    "X = bert_model(input_ids, input_masks_ids)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, input_masks_ids], outputs = X)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "bQ29HkA0MLb7"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, epsilon=1e-06)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3)]\n",
    "\n",
    "model.compile(optimizer, loss, metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Y5LwIjVOmrYX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "13/32 [===========>..................] - ETA: 29:15 - loss: 0.6160 - acc: 0.6614"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c1bdc962c6f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Seu código aqui\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mencoded_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    850\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m/mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/dados/home/wseidel/envs/usp-MAC5725c/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Seu código aqui\n",
    "history = model.fit(\n",
    "    [encoded_train[\"input_ids\"], encoded_train[\"attention_mask\"]],\n",
    "    y_train,\n",
    "    batch_size=256,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=([encoded_test[\"input_ids\"], encoded_test[\"attention_mask\"]], y_test),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z46gCYLT5SCV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lista 10 - BERT Fine-tuning.ipynb - Wesley",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
